{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Data Acquisition","metadata":{}},{"cell_type":"code","source":"# 0) Install\n!pip install yfinance -q\n\n# 1) Imports\nimport os\nfrom datetime import datetime\nimport pandas as pd\nimport yfinance as yf\n\n# 2) Ticker list (50 stocks + 3 indices)\ntickers = [\n    # 50 stocks (sector-balanced sample)\n    \"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"META\",\"NVDA\",\"TSLA\",\"ORCL\",\"INTC\",\"AMD\",\n    \"CSCO\",\"CRM\",\"JPM\",\"BAC\",\"WFC\",\"C\",\"GS\",\"MS\",\"BRK-B\",\"V\",\n    \"MA\",\"JNJ\",\"PFE\",\"MRK\",\"UNH\",\"ABBV\",\"LLY\",\"AMGN\",\"HD\",\n    \"MCD\",\"NKE\",\"SBUX\",\"COST\",\"KO\",\"PEP\",\"WMT\",\"CVS\",\"XOM\",\"CVX\",\n    \"COP\",\"SLB\",\"BA\",\"CAT\",\"GE\",\"HON\",\"AMT\",\"T\",\"VZ\",\"TMUS\"\n]\n\n# 3) Indices\nindices = [\"^GSPC\", \"^IXIC\", \"^DJI\"]\n\nall_tickers = tickers + indices\n\n# 4) Params\nstart = \"2015-01-01\"\nend   = \"2024-12-31\"\ninterval = \"1d\"   # daily\n\n# 5) Fetch data\nprint(f\"Downloading {len(all_tickers)} tickers from {start} to {end} ...\")\nraw = yf.download(\n    tickers = all_tickers,\n    start = start,\n    end = end,\n    interval = interval,\n    group_by = \"ticker\",\n    auto_adjust = True,   # adjust for splits/dividends -> easier modeling\n    threads = True,\n    progress = True\n)\n\n# 6) Normalize into dictionary of DataFrames (OHLCV) per ticker\nos.makedirs(\"/content/data\", exist_ok=True)\ndata_dict = {}\n\ndef extract_df(all_df, symbol):\n    # yfinance returns a MultiIndex column when multiple tickers requested\n    if isinstance(all_df.columns, pd.MultiIndex):\n        # columns like (symbol, 'Open'), (symbol, 'High'), ...\n        df = all_df[symbol].copy()\n    else:\n        # single ticker case\n        df = all_df.copy()\n    # ensure standard columns exist\n    expected = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n    missing = [c for c in expected if c not in df.columns]\n    # If Adj Close was used by auto_adjust, Close exists; if not, try Adj Close\n    if \"Close\" not in df.columns and \"Adj Close\" in df.columns:\n        df = df.rename(columns={\"Adj Close\":\"Close\"})\n    return df\n\nfor sym in all_tickers:\n    try:\n        df = extract_df(raw, sym)\n    except Exception:\n        # fallback: try downloading single ticker (robustness)\n        df = yf.download(sym, start=start, end=end, interval=interval, auto_adjust=True)\n    # Basic cleaning\n    df.index = pd.to_datetime(df.index)\n    # reorder columns if present\n    cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n    df = df[cols].copy()\n    data_dict[sym] = df\n    # Save per-ticker csv\n    df.to_csv(f\"/content/data/{sym.replace('^','_')}.csv\")\n\nprint(\"Saved per-ticker CSVs to /content/data/\")\n\n# 7) Quick integrity report\nreport_rows = []\nfor sym, df in data_dict.items():\n    n = len(df)\n    n_missing = df.isna().sum().sum()\n    zero_vol = int((df.get(\"Volume\", pd.Series(dtype=float)) == 0).sum()) if \"Volume\" in df.columns else 0\n    first, last = (df.index.min(), df.index.max()) if n>0 else (None,None)\n    report_rows.append((sym, n, n_missing, zero_vol, first, last))\n\nreport = pd.DataFrame(report_rows, columns=[\"ticker\",\"n_rows\",\"n_missing_cells\",\"zero_volume_rows\",\"first_date\",\"last_date\"])\ndisplay(report.sort_values(\"ticker\").reset_index(drop=True))\n\n# 9) Create a combined parquet (long format) for faster I/O later\nrows = []\nfor sym, df in data_dict.items():\n    if df.empty:\n        continue\n    tmp = df.reset_index().rename(columns={\"index\":\"date\"})\n    tmp[\"ticker\"] = sym\n    rows.append(tmp)\n\ncombined = pd.concat(rows, ignore_index=True)\ncombined.to_parquet(\"/content/data/combined_ohlcv.parquet\", index=False)\nprint(\"Combined dataset saved to /content/data/combined_ohlcv.parquet\")\nprint(\"Combined shape:\", combined.shape)\n\n# 10) Final checks: overall missing & sample\nprint(\"Overall missing cells:\", combined.isna().sum().sum())\nprint(\"Sample rows:\")\ndisplay(combined.head())","metadata":{"id":"v5arLq0JFQfA","outputId":"7d8ad393-298d-4330-f79c-d0903f9992be","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T15:50:49.598403Z","iopub.execute_input":"2026-01-09T15:50:49.599317Z","iopub.status.idle":"2026-01-09T15:51:00.062848Z","shell.execute_reply.started":"2026-01-09T15:50:49.599288Z","shell.execute_reply":"2026-01-09T15:51:00.062149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Preprocessing\n","metadata":{"id":"jxvwFkMmtjqi"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\n# Load combined dataset\ncombined = pd.read_parquet(\"/content/data/combined_ohlcv.parquet\")\n\n# 1) Fill missing values with 5-day rolling mean\ncombined[['Open','High','Low','Close','Volume']] = combined.groupby('ticker')[\n    ['Open','High','Low','Close','Volume']\n].transform(lambda x: x.fillna(x.rolling(5, min_periods=1).mean()))\n\n# 2) Drop duplicates and zero-volume rows\ncombined = combined.drop_duplicates()\ncombined = combined[combined['Volume'] != 0]\n\n# 2.5) Add Volume_log\ncombined['Volume_log'] = np.log1p(combined['Volume'])  # log1p = log(1 + Volume)\n\n# 3) Sort by time (Important!)\ncombined = combined.sort_values(['ticker','Date'])\n\n# ============================================================================\n# 4) Add 3 new parameters (must be calculated by ticker group to ensure temporal compliance)\n# ============================================================================\n\nprint(\"Starting calculation of 3 new parameters...\")\n\n# 4.1 Calculate returns (for volatility and momentum)\ncombined['Returns'] = combined.groupby('ticker')['Close'].pct_change()\n\n# 4.2 Parameter 1: volatility_ratio_5_20 (5-day / 20-day volatility ratio)\ndef calculate_volatility_ratio(group):\n    \"\"\"Calculate volatility ratio, ensuring temporal compliance\"\"\"\n    returns = group['Returns']\n\n    # 5-day volatility (rolling(5) uses current and previous 4 days of data)\n    vol_5 = returns.rolling(window=5, min_periods=3).std()\n\n    # 20-day volatility (rolling(20) uses current and previous 19 days of data)\n    vol_20 = returns.rolling(window=20, min_periods=10).std()\n\n    # Calculate ratio (avoid division by zero)\n    ratio = vol_5 / vol_20.replace(0, np.nan)\n    return ratio\n\ncombined['volatility_ratio_5_20'] = combined.groupby('ticker').apply(\n    lambda x: calculate_volatility_ratio(x)\n).reset_index(level=0, drop=True)\n\nprint(\"  ‚úÖ Completed volatility_ratio_5_20\")\n\n# 4.3 Parameter 2: momentum_5_sign_strength (5-day momentum strength adjusted by volatility)\ndef calculate_momentum_strength(group):\n    \"\"\"Calculate momentum strength, ensuring temporal compliance\"\"\"\n    close = group['Close']\n    returns = group['Returns']\n\n    # 5-day return (uses data from t-5 to t)\n    returns_5d = (close - close.shift(5)) / close.shift(5)\n\n    # 5-day volatility (uses returns from t-4 to t)\n    vol_5d = returns.rolling(window=5, min_periods=3).std()\n\n    # Momentum strength = 5-day return / 5-day volatility (return per unit volatility)\n    # Handle division by zero and extreme values\n    with np.errstate(divide='ignore', invalid='ignore'):\n        strength = returns_5d / vol_5d.replace(0, np.nan)\n\n    # Clip extreme values\n    strength = strength.clip(lower=-5, upper=5)\n    return strength\n\ncombined['momentum_5_sign_strength'] = combined.groupby('ticker').apply(\n    lambda x: calculate_momentum_strength(x)\n).reset_index(level=0, drop=True)\n\nprint(\"  ‚úÖ Completed momentum_5_sign_strength\")\n\n# 4.4 Parameter 3: volume_price_divergence (simple divergence indicator)\ndef calculate_volume_divergence(group):\n    \"\"\"Calculate volume-price divergence, ensuring temporal compliance\"\"\"\n    close = group['Close']\n    volume_log = group['Volume_log']\n\n    # 5-day price momentum (uses data from t-5 to t)\n    price_momentum_5 = (close - close.shift(5)) / close.shift(5)\n\n    # 5-day volume momentum (uses mean of t-5 to t-1 data, avoids future data)\n    # Note: cannot use today's volume to calculate momentum since it's already known\n    volume_mean_5 = volume_log.shift(1).rolling(window=5, min_periods=3).mean()\n    current_volume = volume_log\n    volume_momentum_5 = (current_volume - volume_mean_5) / volume_mean_5.replace(0, np.nan)\n\n    # Simple divergence indicator: price momentum √ó volume momentum\n    # Positive: price and volume rise together (healthy uptrend)\n    # Negative: price-volume divergence (potential reversal)\n    divergence = price_momentum_5 * volume_momentum_5\n\n    # Clip extreme values\n    divergence = divergence.clip(lower=-1, upper=1)\n    return divergence\n\ncombined['volume_price_divergence'] = combined.groupby('ticker').apply(\n    lambda x: calculate_volume_divergence(x)\n).reset_index(level=0, drop=True)\n\nprint(\"  ‚úÖ Completed volume_price_divergence\")\nprint(\"‚úÖ All 3 new parameters calculated\")\n\n# ============================================================================\n# 5) Create two Y labels: binary classification (volatility presence) and ternary classification (volatility direction)\n# ============================================================================\n\nprint(\"\\nCreating two Y labels...\")\n\ndef create_volatility_targets(combined, threshold=0.025):\n    \"\"\"\n    Create two target variables:\n    1. y_binary: binary classification, 0=calm, 1=significant volatility (up or down)\n    2. y_3class: ternary classification, 0=calm, 1=big rise, 2=big drop\n    \"\"\"\n    combined = combined.sort_values(['ticker','Date']).copy()\n\n    # Get closing prices for the next 1 and 2 days\n    price_tomorrow = combined.groupby('ticker')['Close'].shift(-1)\n    price_day_after = combined.groupby('ticker')['Close'].shift(-2)\n\n    # Calculate returns relative to today's closing price\n    return_tomorrow = (price_tomorrow - combined['Close']) / combined['Close']\n    return_day_after = (price_day_after - combined['Close']) / combined['Close']\n\n    # Find maximum rise and maximum drop within the next 2 days\n    max_up = pd.concat([return_tomorrow, return_day_after], axis=1).max(axis=1)\n    max_down = pd.concat([return_tomorrow, return_day_after], axis=1).min(axis=1)\n\n    # ========== Create binary classification Y: whether there is significant volatility ==========\n    # Significant volatility = any day's absolute price change > threshold within next 2 days\n    y_binary = ((max_up > threshold) | (max_down < -threshold)).astype(int)\n    y_binary.name = 'y_binary'\n\n    # ========== Create ternary classification Y: volatility direction ==========\n    y_3class = pd.Series(0, index=combined.index)  # Default class 0 (calm)\n\n    # Case 1: only big rise (max rise > threshold, and no big drop)\n    condition_up_only = (max_up > threshold) & (max_down >= -threshold)\n    y_3class.loc[condition_up_only] = 1  # big rise\n\n    # Case 2: only big drop (max drop < -threshold, and no big rise)\n    condition_down_only = (max_down < -threshold) & (max_up <= threshold)\n    y_3class.loc[condition_down_only] = 2  # big drop\n\n    # Case 3: both big rise and big drop (high volatility)\n    condition_both = (max_up > threshold) & (max_down < -threshold)\n    if condition_both.any():\n        # Choose the direction with larger magnitude\n        for idx in condition_both[condition_both].index:\n            if abs(max_up.loc[idx]) > abs(max_down.loc[idx]):\n                y_3class.loc[idx] = 1  # rise magnitude is larger\n            else:\n                y_3class.loc[idx] = 2  # drop magnitude is larger\n\n    y_3class.name = 'y_3class'\n\n    # Statistics distribution\n    total = len(y_binary.dropna())\n    if total > 0:\n        print(f\"Binary classification distribution:\")\n        print(f\"  Class 0 (calm): {(y_binary == 0).sum()/total:.2%}\")\n        print(f\"  Class 1 (significant volatility): {(y_binary == 1).sum()/total:.2%}\")\n\n        print(f\"\\nTernary classification distribution:\")\n        class_counts = y_3class.value_counts()\n        print(f\"  Class 0 (calm): {class_counts.get(0, 0)/total:.2%}\")\n        print(f\"  Class 1 (big rise): {class_counts.get(1, 0)/total:.2%}\")\n        print(f\"  Class 2 (big drop): {class_counts.get(2, 0)/total:.2%}\")\n        print(f\"  Total samples: {total}\")\n\n    return y_binary, y_3class\n\n# Create two target variables (2.5% threshold)\ncombined['y_binary'], combined['y_3class'] = create_volatility_targets(combined, threshold=0.025)\n\n# ============================================================================\n# 6) Clean data and define features\n# ============================================================================\n\nprint(\"\\nCleaning data...\")\n\n# Delete rows containing NaN (target variables and important features)\nrequired_columns = ['y_binary', 'y_3class',\n                   'volatility_ratio_5_20', 'momentum_5_sign_strength',\n                   'volume_price_divergence']\n\ncombined = combined.dropna(subset=required_columns).reset_index(drop=True)\n\nprint(f\"Data size after cleaning: {len(combined)} rows\")\n\n# Define feature columns (5 basic + 3 new features)\nbase_features = ['Open', 'High', 'Low', 'Close', 'Volume_log']\nnew_features = ['volatility_ratio_5_20', 'momentum_5_sign_strength', 'volume_price_divergence']\nfeatures = base_features + new_features  # 8 features in total\n\nprint(f\"\\nFeature list ({len(features)} features):\")\nfor i, feat in enumerate(features, 1):\n    print(f\"  {i:2d}. {feat}\")\n\n# ============================================================================\n# 7) Construct 30-day sliding windows (using all features)\n# ============================================================================\n\nwindow_size = 30\nX_list, y_binary_list, y_3class_list = [], [], []\n\nprint(f\"\\nConstructing sliding windows ({window_size} days)...\")\n\nfor ticker, group in combined.groupby('ticker'):\n    group = group.sort_values('Date')\n\n    # Extract feature data\n    data = group[features].values\n    y_binary = group['y_binary'].values\n    y_3class = group['y_3class'].values\n\n    # Create sliding windows (ensure sufficient data)\n    n_windows = len(group) - window_size\n    if n_windows < 1:\n        continue\n\n    for i in range(n_windows):\n        X_list.append(data[i:i+window_size])\n        y_binary_list.append(y_binary[i+window_size])\n        y_3class_list.append(y_3class[i+window_size])\n\n    if len(X_list) % 5000 == 0:\n        print(f\"  Created {len(X_list)} windows...\")\n\nX = np.array(X_list)\ny_binary = np.array(y_binary_list)\ny_3class = np.array(y_3class_list)\n\nprint(\"\\nDataset shapes:\")\nprint(f\"X: {X.shape}  (samples, {window_size} days, {len(features)} features)\")\nprint(f\"y_binary: {y_binary.shape}  (binary classification: 0=calm, 1=significant volatility)\")\nprint(f\"y_3class: {y_3class.shape}  (ternary classification: 0=calm, 1=big rise, 2=big drop)\")\n\n# Binary classification distribution statistics\nbinary_pos = np.sum(y_binary == 1)\nbinary_total = len(y_binary)\nprint(f\"\\nBinary classification distribution:\")\nprint(f\"  Class 0 (calm): {binary_total - binary_pos} samples ({(binary_total - binary_pos)/binary_total:.2%})\")\nprint(f\"  Class 1 (significant volatility): {binary_pos} samples ({binary_pos/binary_total:.2%})\")\n\n# Ternary classification distribution statistics\nunique, counts = np.unique(y_3class, return_counts=True)\nprint(f\"\\nTernary classification distribution:\")\nfor cls, cnt in zip(unique, counts):\n    label = {0: 'Calm', 1: 'Big rise', 2: 'Big drop'}[cls]\n    print(f\"  Class {cls} ({label}): {cnt} samples ({cnt/len(y_3class):.2%})\")\n\n# ============================================================================\n# 8) Split dataset chronologically (training 80%, testing 20%)\n# ============================================================================\n\nsplit_idx = int(len(X) * 0.8)\n\nX_train_full = X[:split_idx]\ny_binary_train_full = y_binary[:split_idx]\ny_3class_train_full = y_3class[:split_idx]\n\nX_test_final = X[split_idx:]\ny_binary_test_final = y_binary[split_idx:]\ny_3class_test_final = y_3class[split_idx:]\n\nprint(f\"\\nSplit results:\")\nprint(f\"Training set: {len(X_train_full)} samples ({len(X_train_full)/len(X)*100:.1f}%)\")\nprint(f\"Test set: {len(X_test_final)} samples ({len(X_test_final)/len(X)*100:.1f}%)\")\n\nprint(f\"\\nTraining set distribution - Binary: calm={np.sum(y_binary_train_full==0)}, significant volatility={np.sum(y_binary_train_full==1)}\")\nprint(f\"Training set distribution - Ternary: calm={np.sum(y_3class_train_full==0)}, big rise={np.sum(y_3class_train_full==1)}, big drop={np.sum(y_3class_train_full==2)}\")\n\n# ============================================================================\n# 9) Fit normalizer only on training set\n# ============================================================================\n\nprint(\"\\nNormalizing...\")\n\nn_train_samples = X_train_full.shape[0]\nn_test_samples = X_test_final.shape[0]\nn_features = len(features)\n\n# Reshape to 2D for normalization\nX_train_2d = X_train_full.reshape(-1, n_features)  # (train_samples*30, 8)\nX_test_2d = X_test_final.reshape(-1, n_features)    # (test_samples*30, 8)\n\n# Create normalizer\nscaler = StandardScaler()\n\n# Fit normalizer only on training set\nX_train_normalized_2d = scaler.fit_transform(X_train_2d)\n\n# Transform test set using training set statistics\nX_test_normalized_2d = scaler.transform(X_test_2d)\n\n# Restore 3D shape\nX_train = X_train_normalized_2d.reshape(n_train_samples, window_size, n_features)\nX_test = X_test_normalized_2d.reshape(n_test_samples, window_size, n_features)\n\nprint(\"‚úÖ Normalization completed\")\nprint(f\"Training set after normalization - Mean: {np.mean(X_train):.4f}, Std: {np.std(X_train):.4f}\")\nprint(f\"Test set after normalization - Mean: {np.mean(X_test):.4f}, Std: {np.std(X_test):.4f}\")\n\n# ============================================================================\n# 10) Save normalizer and feature list\n# ============================================================================\n\nimport joblib\n\njoblib.dump(scaler, 'scaler_volatility.pkl')\njoblib.dump(features, 'features_volatility.pkl')\n\nprint(\"\\n‚úÖ Data preparation completed!\")\nprint(\"‚úÖ Normalizer saved as: scaler_volatility.pkl\")\nprint(\"‚úÖ Feature list saved as: features_volatility.pkl\")\nprint(f\"‚úÖ You can now train a model using {len(features)} features\")\n\n# ============================================================================\n# Usage instructions\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Data Preparation Complete - Usage Instructions\")\nprint(\"=\"*60)\nprint(\"Available datasets:\")\nprint(\"\\n1. Binary classification model (detect significant volatility):\")\nprint(\"   Features: X_train, X_test\")\nprint(\"   Labels: y_binary_train_full, y_binary_test_final\")\nprint(\"   Description: 0=calm(no 2.5%+ volatility in next 2 days), 1=significant volatility\")\nprint(\"   Suitable for: simple volatility detection, use 'binary_crossentropy' loss function\")\nprint(\"   Output layer: Dense(1, activation='sigmoid')\")\nprint(\"\")\nprint(\"2. Ternary classification model (predict volatility direction):\")\nprint(\"   Features: X_train, X_test\")\nprint(\"   Labels: y_3class_train_full, y_3class_test_final\")\nprint(\"   Description: 0=calm, 1=big rise(>2.5%), 2=big drop(<-2.5%)\")\nprint(\"   Suitable for: directional trading strategies, use 'categorical_crossentropy' loss function\")\nprint(\"   Output layer: Dense(3, activation='softmax')\")\nprint(\"\")\nprint(\"3. Feature descriptions:\")\nfor i, feat in enumerate(features, 1):\n    feat_desc = {\n        'Open': 'Opening price',\n        'High': 'Highest price',\n        'Low': 'Lowest price',\n        'Close': 'Closing price',\n        'Volume_log': 'Logarithm of trading volume',\n        'volatility_ratio_5_20': '5-day / 20-day volatility ratio',\n        'momentum_5_sign_strength': '5-day momentum strength adjusted by volatility',\n        'volume_price_divergence': 'Volume-price divergence indicator'\n    }\n    print(f\"   {i:2d}. {feat:25s} - {feat_desc.get(feat, '')}\")","metadata":{"id":"lxsghe92N2tn","outputId":"834552a3-e9fb-4c63-960e-9f89480950ff","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T15:51:03.528949Z","iopub.execute_input":"2026-01-09T15:51:03.529228Z","iopub.status.idle":"2026-01-09T15:51:05.183098Z","shell.execute_reply.started":"2026-01-09T15:51:03.529199Z","shell.execute_reply":"2026-01-09T15:51:05.182317Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. LSTM Scenario 1\n","metadata":{"id":"kFmZHRbz6d5J"}},{"cell_type":"markdown","source":"class 0: No significant fluctuations expected in the next 2 days\n\nclass 1: Significant fluctuations expected in the next 2 days","metadata":{"id":"qfQrVBJseUhS"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.utils.class_weight import compute_class_weight\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# Practical Strategy: Find High Precision Threshold on Validation Set\n# Goal: Validation set precision ‚â• 70%, Test set precision ‚â• 60%\n# ============================================================================\n\nprint(\"=\"*60)\nprint(\"Practical High Precision Threshold Strategy\")\nprint(\"Strategy: Find threshold achieving precision ‚â• 70% on validation set\")\nprint(\"Expectation: Achieve precision ‚â• 60% on test set\")\nprint(\"=\"*60)\n\n# ============================================================================\n# 1. Data Preparation\n# ============================================================================\n\ny_train_full = y_binary_train_full.copy()\ny_test_final = y_binary_test_final.copy()\n\nprint(f\"Data Distribution:\")\nprint(f\"  Test Set: Calm={np.sum(y_test_final==0)} ({np.mean(y_test_final==0):.2%}), \"\n      f\"High Volatility={np.sum(y_test_final==1)} ({np.mean(y_test_final==1):.2%})\")\n\n# Split training and validation sets\ntrain_ratio = 0.8\ntrain_idx = int(len(X_train) * train_ratio)\n\nX_train_final = X_train[:train_idx]\nX_val = X_train[train_idx:]\ny_train_final = y_train_full[:train_idx]\ny_val = y_train_full[train_idx:]\n\nprint(f\"\\nData Split:\")\nprint(f\"  Training Set: {len(X_train_final)} samples\")\nprint(f\"  Validation Set: {len(X_val)} samples (for threshold selection)\")\nprint(f\"  Test Set: {len(X_test)} samples (for final evaluation)\")\n\n# ============================================================================\n# 2. Train Model (or Load Existing Model)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Model Training\")\nprint(\"=\"*60)\n\ntry:\n    # Try to load an existing model\n    print(\"Attempting to load existing model...\")\n    model = tf.keras.models.load_model('precision_optimized_trading_model.keras')\n    print(\"‚úÖ Model loaded successfully\")\nexcept:\n    print(\"‚ö†Ô∏è  Unable to load model, training a new model...\")\n    \n    def create_simple_model():\n        model = Sequential([\n            LSTM(64, input_shape=(window_size, len(features))),\n            Dropout(0.3),\n            BatchNormalization(),\n            Dense(32, activation='relu'),\n            Dropout(0.2),\n            Dense(16, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ])\n        \n        model.compile(\n            optimizer=Adam(learning_rate=0.0005),\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n        return model\n    \n    model = create_simple_model()\n    \n    # Train the model\n    print(\"Training model...\")\n    history = model.fit(\n        X_train_final, y_train_final,\n        validation_data=(X_val, y_val),\n        epochs=20,\n        batch_size=64,\n        verbose=1,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=5,\n                restore_best_weights=True,\n                verbose=1\n            )\n        ]\n    )\n\n# ============================================================================\n# 3. Find Threshold Achieving ‚â•70% Precision on Validation Set (Key Step)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Find Threshold Achieving ‚â•70% Precision on Validation Set\")\nprint(\"=\"*60)\n\n# Get prediction probabilities on validation set\ny_val_proba = model.predict(X_val, verbose=0, batch_size=128).flatten()\n\n# Find all thresholds achieving precision ‚â• 70%\nthresholds = np.arange(0.5, 0.95, 0.005)  # Start search from 0.5\nval_results = []\n\nprint(\"Searching thresholds...\")\nfor thresh in thresholds:\n    y_val_pred = (y_val_proba > thresh).astype(int)\n    \n    if np.sum(y_val_pred == 1) > 0:\n        precision = np.mean(y_val[y_val_pred == 1] == 1)\n        recall = np.mean(y_val_pred[y_val == 1] == 1) if np.sum(y_val == 1) > 0 else 0\n        signals = np.sum(y_val_pred == 1)\n        \n        val_results.append({\n            'threshold': thresh,\n            'precision': precision,\n            'recall': recall,\n            'signals': signals,\n            'signal_ratio': signals / len(y_val)\n        })\n\nval_results_df = pd.DataFrame(val_results)\n\n# Find all thresholds with precision ‚â• 70%\nhigh_precision_thresholds = val_results_df[val_results_df['precision'] >= 0.7].copy()\n\nif len(high_precision_thresholds) > 0:\n    print(f\"\\n‚úÖ Found {len(high_precision_thresholds)} thresholds achieving ‚â•70% precision\")\n    \n    # Select the threshold with the highest number of signals (while maintaining precision)\n    high_precision_thresholds = high_precision_thresholds.sort_values('signals', ascending=False)\n    \n    print(f\"\\nThresholds achieving ‚â•70% precision on validation set (sorted by signal count):\")\n    print(f\"{'Threshold':<8} {'Precision':<10} {'Signals':<10} {'Signal Ratio':<10}\")\n    print(\"-\" * 45)\n    \n    for i, row in high_precision_thresholds.head(10).iterrows():\n        print(f\"{row['threshold']:.3f}     {row['precision']:.2%}     \"\n              f\"{int(row['signals']):<10} {row['signal_ratio']:.2%}\")\n    \n    # Threshold selection strategy\n    print(f\"\\nüîç Threshold Selection Strategy:\")\n    \n    # Strategy 1: Select threshold with the most signals (most practical)\n    selected_threshold = high_precision_thresholds.iloc[0]['threshold']\n    val_precision = high_precision_thresholds.iloc[0]['precision']\n    val_signals = int(high_precision_thresholds.iloc[0]['signals'])\n    \n    print(f\"Strategy 1 - Maximize Signal Count:\")\n    print(f\"  Threshold: {selected_threshold:.3f}\")\n    print(f\"  Validation Set Precision: {val_precision:.2%}\")\n    print(f\"  Validation Set Signals: {val_signals} ({high_precision_thresholds.iloc[0]['signal_ratio']:.2%})\")\n    \n    # If too few signals, try slightly lower precision but more signals\n    if val_signals < 100:\n        print(f\"\\n‚ö†Ô∏è  Low signal count ({val_signals}), relaxing conditions...\")\n        \n        # Find thresholds achieving precision ‚â• 65%\n        medium_precision_thresholds = val_results_df[val_results_df['precision'] >= 0.65].copy()\n        if len(medium_precision_thresholds) > 0:\n            medium_precision_thresholds = medium_precision_thresholds.sort_values('signals', ascending=False)\n            selected_threshold = medium_precision_thresholds.iloc[0]['threshold']\n            val_precision = medium_precision_thresholds.iloc[0]['precision']\n            val_signals = int(medium_precision_thresholds.iloc[0]['signals'])\n            \n            print(f\"Strategy 2 - Precision ‚â• 65%:\")\n            print(f\"  Threshold: {selected_threshold:.3f}\")\n            print(f\"  Validation Set Precision: {val_precision:.2%}\")\n            print(f\"  Validation Set Signals: {val_signals} ({medium_precision_thresholds.iloc[0]['signal_ratio']:.2%})\")\n    \nelse:\n    print(f\"\\n‚ö†Ô∏è  No threshold found achieving ‚â•70% precision\")\n    print(f\"  Maximum precision on validation set: {val_results_df['precision'].max():.2%}\")\n    \n    # Find thresholds achieving precision ‚â• 65%\n    medium_precision_thresholds = val_results_df[val_results_df['precision'] >= 0.65].copy()\n    if len(medium_precision_thresholds) > 0:\n        medium_precision_thresholds = medium_precision_thresholds.sort_values('signals', ascending=False)\n        selected_threshold = medium_precision_thresholds.iloc[0]['threshold']\n        val_precision = medium_precision_thresholds.iloc[0]['precision']\n        val_signals = int(medium_precision_thresholds.iloc[0]['signals'])\n        \n        print(f\"Using threshold achieving ‚â•65% precision:\")\n        print(f\"  Threshold: {selected_threshold:.3f}\")\n        print(f\"  Validation Set Precision: {val_precision:.2%}\")\n        print(f\"  Validation Set Signals: {val_signals} ({medium_precision_thresholds.iloc[0]['signal_ratio']:.2%})\")\n    else:\n        # Use threshold with highest precision\n        best_row = val_results_df.loc[val_results_df['precision'].idxmax()]\n        selected_threshold = best_row['threshold']\n        val_precision = best_row['precision']\n        val_signals = int(best_row['signals'])\n        \n        print(f\"Using threshold with highest precision:\")\n        print(f\"  Threshold: {selected_threshold:.3f}\")\n        print(f\"  Validation Set Precision: {val_precision:.2%}\")\n        print(f\"  Validation Set Signals: {val_signals} ({best_row['signal_ratio']:.2%})\")\n\n# ============================================================================\n# 4. Evaluate Selected Threshold on Test Set\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Evaluate Selected Threshold on Test Set\")\nprint(f\"Threshold: {selected_threshold:.3f} (from validation set)\")\nprint(\"=\"*60)\n\n# Get prediction probabilities on test set\ny_test_proba = model.predict(X_test, verbose=0, batch_size=128).flatten()\n\n# Use the threshold selected from validation set\ny_test_pred = (y_test_proba > selected_threshold).astype(int)\n\n# Calculate test set metrics\ntest_precision = precision_score(y_test_final, y_test_pred, zero_division=0)\ntest_recall = recall_score(y_test_final, y_test_pred, zero_division=0)\ntest_f1 = f1_score(y_test_final, y_test_pred, zero_division=0)\ntest_accuracy = accuracy_score(y_test_final, y_test_pred)\ntest_auc = roc_auc_score(y_test_final, y_test_proba)\ntest_signals = np.sum(y_test_pred == 1)\n\nprint(f\"Test Set Performance:\")\nprint(f\"  Threshold: {selected_threshold:.3f}\")\nprint(f\"  Precision: {test_precision:.4f} (Target: ‚â•60%)\")\nprint(f\"  Recall: {test_recall:.4f}\")\nprint(f\"  F1 Score: {test_f1:.4f}\")\nprint(f\"  Accuracy: {test_accuracy:.4f}\")\nprint(f\"  AUC: {test_auc:.4f}\")\nprint(f\"  Signal Count: {test_signals} ({test_signals/len(y_test_final):.2%})\")\n\n# Check if target is achieved\nif test_precision >= 0.6:\n    print(f\"\\n‚úÖ Success! Test set precision ‚â• 60% ({test_precision:.2%})\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Target not reached! Test set precision: {test_precision:.2%} (< 60%)\")\n\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test_final, y_test_pred,\n                           target_names=['Calm', 'High Volatility']))\n\n# Confusion matrix\ncm_test = confusion_matrix(y_test_final, y_test_pred)\nprint(f\"Confusion Matrix:\")\nprint(f\"               Predicted Calm    Predicted High Volatility\")\nprint(f\"Actual Calm      {cm_test[0,0]:8d}               {cm_test[0,1]:8d}\")\nprint(f\"Actual High Volatility    {cm_test[1,0]:8d}               {cm_test[1,1]:8d}\")\n\n# ============================================================================\n# 5. Visualization Analysis\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Visualization Analysis\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# 1. Validation Set Precision vs Threshold Curve\naxes[0, 0].plot(val_results_df['threshold'], val_results_df['precision'], 'b-', linewidth=2)\naxes[0, 0].axhline(y=0.7, color='red', linestyle='--', alpha=0.5, label='Target Line (70%)')\naxes[0, 0].axvline(x=selected_threshold, color='green', linestyle='--', \n                  label=f'Selected Threshold ({selected_threshold:.3f})')\naxes[0, 0].set_title('Validation Set: Precision vs Threshold')\naxes[0, 0].set_xlabel('Threshold')\naxes[0, 0].set_ylabel('Precision')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# 2. Validation Set Signal Count vs Threshold Curve\naxes[0, 1].plot(val_results_df['threshold'], val_results_df['signals'], 'g-', linewidth=2)\naxes[0, 1].axvline(x=selected_threshold, color='green', linestyle='--')\naxes[0, 1].set_title('Validation Set: Signal Count vs Threshold')\naxes[0, 1].set_xlabel('Threshold')\naxes[0, 1].set_ylabel('Signal Count')\naxes[0, 1].grid(True)\n\n# 3. Test Set Prediction Probability Distribution\naxes[0, 2].hist(y_test_proba[y_test_final == 0], alpha=0.5, label='Actual Calm', bins=50, color='blue')\naxes[0, 2].hist(y_test_proba[y_test_final == 1], alpha=0.5, label='Actual High Volatility', bins=50, color='red')\naxes[0, 2].axvline(x=selected_threshold, color='green', linestyle='--', linewidth=2, \n                  label=f'Threshold={selected_threshold:.3f}')\naxes[0, 2].set_title('Test Set: Prediction Probability Distribution')\naxes[0, 2].set_xlabel('Probability of High Volatility')\naxes[0, 2].set_ylabel('Sample Count')\naxes[0, 2].legend()\naxes[0, 2].grid(True)\n\n# 4. ROC Curve\nfrom sklearn.metrics import roc_curve\nfpr, tpr, _ = roc_curve(y_test_final, y_test_proba)\naxes[1, 0].plot(fpr, tpr, 'b-', label=f'AUC={test_auc:.3f}')\naxes[1, 0].plot([0, 1], [0, 1], 'k--')\naxes[1, 0].set_title('ROC Curve')\naxes[1, 0].set_xlabel('False Positive Rate')\naxes[1, 0].set_ylabel('True Positive Rate')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# 5. Precision-Recall Curve\nfrom sklearn.metrics import precision_recall_curve\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test_final, y_test_proba)\naxes[1, 1].plot(recall_curve, precision_curve, 'purple', linewidth=2)\naxes[1, 1].axhline(y=0.6, color='red', linestyle='--', alpha=0.5, label='Target Line (60%)')\naxes[1, 1].scatter([test_recall], [test_precision], color='green', s=100, marker='o', \n                  label=f'Current Point ({test_recall:.2f}, {test_precision:.2f})')\naxes[1, 1].set_title('Precision-Recall Curve')\naxes[1, 1].set_xlabel('Recall')\naxes[1, 1].set_ylabel('Precision')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\n# 6. Performance Comparison\ncategories = ['Validation Precision', 'Test Precision', 'Validation Signals/100', 'Test Signals/100']\nvalues = [val_precision, test_precision, val_signals/100, test_signals/100]\ncolors = ['blue', 'red', 'green', 'orange']\n\nbars = axes[1, 2].bar(categories, values, color=colors)\naxes[1, 2].set_title('Validation Set vs Test Set Performance Comparison')\naxes[1, 2].set_ylabel('Value (Precision as %, Signals/100)')\naxes[1, 2].axhline(y=0.7, color='red', linestyle='--', alpha=0.5, label='70% Precision Target')\naxes[1, 2].axhline(y=0.6, color='orange', linestyle='--', alpha=0.5, label='60% Precision Target')\naxes[1, 2].legend()\n\n# Add value labels on bars\nfor bar, value in zip(bars, values):\n    height = bar.get_height()\n    axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n                   f'{value:.2f}' if value < 1 else f'{int(value*100)}',\n                   ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# 6. Save Model and Configuration\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Save Model and Configuration\")\nprint(\"=\"*60)\n\n# ============================================================================\n# 6. ‰∫§ÊòìÊÄßËÉΩÂàÜÊûê\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Trading Performance Analysis\")\nprint(\"=\"*60)\n\n# È¢ÑÂÖàÂÆö‰πâÂèòÈáèÔºåÈÅøÂÖçÂêéÁª≠ÂºïÁî®ÈîôËØØ\nexpected_return_per_trade = 0\n\nprint(f\"üéØ Trading Signal Quality:\")\nprint(f\"  Total Signals: {test_signals}\")\nprint(f\"  Correct Signals: {cm_test[1,1]} (Success Rate: {test_precision:.1%})\")\nprint(f\"  Wrong Signals: {cm_test[0,1]} (Risk Rate: {cm_test[0,1]/test_signals if test_signals>0 else 0:.1%})\")\nprint(f\"  Missed Opportunities: {cm_test[1,0]} (Opportunity Cost)\")\n\nif test_signals > 0:\n    # ÁÆÄÂçïÊî∂ÁõäËÆ°ÁÆó\n    win_rate = test_precision\n    loss_rate = 1 - win_rate\n    avg_win = 0.025  # Âπ≥ÂùáÁõàÂà©2.5%\n    avg_loss = 0.015  # Âπ≥Âùá‰∫èÊçü1.5%\n    \n    expected_return_per_trade = win_rate * avg_win - loss_rate * avg_loss\n    total_expected_return = expected_return_per_trade * test_signals\n    \n    print(f\"\\nüí∞ Expected Returns:\")\n    print(f\"  Expected Return Per Trade: {expected_return_per_trade:.2%}\")\n    print(f\"  Total Expected Return: {total_expected_return:.2%}\")\n    print(f\"  Estimated Annual Trade Count: {test_signals/(len(y_test_final)/252):.0f} trades\")\n    print(f\"  Annualized Expected Return: {expected_return_per_trade * (test_signals/(len(y_test_final)/252)):.1%}\")\n    \n    if expected_return_per_trade > 0:\n        print(f\"  ‚úÖ Positive expected return, model has trading value\")\n    else:\n        print(f\"  ‚ö†Ô∏è  Negative expected return, needs optimization\")\n\n# ============================================================================\n# 7. ‰øùÂ≠òÊ®°ÂûãÂíåÈÖçÁΩÆ\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Save Model and Configuration\")\nprint(\"=\"*60)\n\n# save model\nmodel.save(f'practical_model_threshold_{selected_threshold:.3f}.keras')\nprint(f\"‚úÖ Model saved: practical_model_threshold_{selected_threshold:.3f}.keras\")\n\n# ‰øùÂ≠òÈÖçÁΩÆ\nimport joblib\n\nif 'total_expected_return' not in locals():\n    total_expected_return = 0\n\nif 'test_signals' not in locals():\n    test_signals = 0\n\nif 'cm_test' not in locals():\n    cm_test = np.array([[0, 0], [0, 0]])\n\nconfig = {\n    'strategy': 'Find ‚â•70% precision threshold on validation set, expecting ‚â•60% on test set',\n    'threshold_selection': {\n        'source': 'validation_set',\n        'min_precision_target': 0.7,\n        'selected_threshold': float(selected_threshold),\n        'validation_performance': {\n            'precision': float(val_precision),\n            'recall': float(val_results_df[val_results_df['threshold'] == selected_threshold]['recall'].iloc[0]),\n            'signals': int(val_signals),\n            'signal_ratio': float(val_results_df[val_results_df['threshold'] == selected_threshold]['signal_ratio'].iloc[0])\n        }\n    },\n    'test_performance': {\n        'precision': float(test_precision),\n        'recall': float(test_recall),\n        'f1': float(test_f1),\n        'accuracy': float(test_accuracy),\n        'auc': float(test_auc),\n        'signals': int(test_signals) if 'test_signals' in locals() else 0,\n        'signal_ratio': float(test_signals / len(y_test_final)) if 'test_signals' in locals() and test_signals > 0 else 0\n    },\n    'trading_analysis': {\n        'expected_return_per_trade': float(expected_return_per_trade) if test_signals>0 else 0,\n        'total_expected_return': float(total_expected_return),\n        'win_rate': float(test_precision),\n        'total_signals': int(test_signals) if 'test_signals' in locals() else 0,\n        'correct_signals': int(cm_test[1,1]) if cm_test.size > 1 else 0,\n        'wrong_signals': int(cm_test[0,1]) if cm_test.size > 1 else 0,\n        'annualized_trades': float(test_signals/(len(y_test_final)/252)) if 'test_signals' in locals() and test_signals > 0 else 0\n    }\n}\njoblib.dump(config, f'practical_model_config_{selected_threshold:.3f}.pkl')\nprint(f\"‚úÖ Configuration saved: practical_model_config_{selected_threshold:.3f}.pkl\")\n\n# ============================================================================\n# 7. Final Recommendations\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Final Recommendations\")\nprint(\"=\"*60)\n\nprint(f\"üéØ Strategy Summary:\")\nprint(f\"  1. Find threshold achieving ‚â•70% precision on validation set\")\nprint(f\"  2. Selected threshold: {selected_threshold:.3f}\")\nprint(f\"  3. Validation set precision: {val_precision:.1%}\")\nprint(f\"  4. Test set precision: {test_precision:.1%}\")\n\nprint(f\"\\nüìä Performance Evaluation:\")\nif test_precision >= 0.6:\n    print(f\"  ‚úÖ Target achieved! Test set precision ‚â• 60%\")\n    print(f\"  ‚úÖ Model can be used for actual trading\")\n    \nelse:\n    print(f\"  ‚ö†Ô∏è  Target not reached! Test set precision: {test_precision:.1%} (< 60%)\")\n    print(f\"  ‚ö†Ô∏è  Further optimization needed\")\n    \n    print(f\"\\nüí° Optimization Suggestions:\")\n    print(f\"  1. Try higher thresholds for higher precision\")\n    print(f\"  2. Retrain model, adjust class weights\")\n    print(f\"  3. Improve feature engineering\")\n\nprint(f\"\\nüîß Usage Instructions:\")\nprint(f\"  1. Load model: practical_model_threshold_{selected_threshold:.3f}.keras\")\nprint(f\"  2. Predict on new data: probabilities = model.predict(X_new)\")\nprint(f\"  3. Generate trading signals: signals = probabilities > {selected_threshold:.3f}\")\n\nprint(f\"\\n‚úÖ Practical High Precision Strategy Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:07:14.077308Z","iopub.execute_input":"2026-01-09T16:07:14.077631Z","iopub.status.idle":"2026-01-09T16:09:19.600613Z","shell.execute_reply.started":"2026-01-09T16:07:14.077606Z","shell.execute_reply":"2026-01-09T16:09:19.599861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. LSTM Scenario 2","metadata":{"id":"yrvtCQo3MHxv"}},{"cell_type":"markdown","source":"class 0: No significant fluctuations expected in the next 2 days\n\nclass 1: Significant fluctuations expected (upward trend) in the next 2 days\n\nclass 2: Significant fluctuations expected (downward trend) in the next 2 days","metadata":{"id":"SEbiTNR8eijY"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils.class_weight import compute_class_weight\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# Practical Ternary Classification Strategy: Optimization Based on 0.4 Baseline Threshold\n# Goal: Precision for both big rise (1) and big drop (2) predictions ‚â•55%\n# Baseline threshold: 0.4 (more suitable for ternary classification and imbalanced data scenarios)\n# ============================================================================\n\nprint(\"=\"*60)\nprint(\"Ternary Classification High Precision Threshold Strategy - Based on 0.4 Baseline\")\nprint(\"Strategy: Find thresholds achieving ‚â•55% precision for both big rise and big drop on validation set\")\nprint(\"Baseline threshold: 0.4 (more reasonable starting point for ternary classification)\")\nprint(\"Goal: Achieve ‚â•55% precision for both big rise and big drop on test set\")\nprint(\"=\"*60)\n\n# ============================================================================\n# 1. Data Preparation - Ternary Classification\n# ============================================================================\n\ny_train_full = y_3class_train_full.copy()  # 0=calm, 1=big rise, 2=big drop\ny_test_final = y_3class_test_final.copy()\n\n# Convert to one-hot encoding for model training\ny_train_onehot = to_categorical(y_train_full, num_classes=3)\n\nprint(f\"Data Distribution:\")\nprint(f\"Training Set Label Distribution:\")\nfor i, label in enumerate(['Calm', 'Big Rise', 'Big Drop']):\n    count = np.sum(y_train_full == i)\n    print(f\"  Class {i}({label}): {count} samples ({count/len(y_train_full):.2%})\")\n\nprint(f\"\\nTest Set Label Distribution:\")\nfor i, label in enumerate(['Calm', 'Big Rise', 'Big Drop']):\n    count = np.sum(y_test_final == i)\n    print(f\"  Class {i}({label}): {count} samples ({count/len(y_test_final):.2%})\")\n\n# Split training and validation sets\ntrain_ratio = 0.8\ntrain_idx = int(len(X_train) * train_ratio)\n\nX_train_final = X_train[:train_idx]\nX_val = X_train[train_idx:]\ny_train_final = y_train_full[:train_idx]\ny_val = y_train_full[train_idx:]\ny_train_onehot_final = y_train_onehot[:train_idx]\ny_val_onehot = y_train_onehot[train_idx:]\n\nprint(f\"\\nData Split:\")\nprint(f\"  Training Set: {len(X_train_final)} samples\")\nprint(f\"  Validation Set: {len(X_val)} samples (for threshold selection)\")\nprint(f\"  Test Set: {len(X_test)} samples (for final evaluation)\")\n\n# ============================================================================\n# 2. Train Model (or Load Existing Model)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Model Training - Ternary Classification\")\nprint(\"=\"*60)\n\ntry:\n    # Try to load existing model\n    print(\"Attempting to load existing model...\")\n    model = tf.keras.models.load_model('ternary_classification_trading_model.keras')\n    print(\"‚úÖ Model loaded successfully\")\nexcept:\n    print(\"‚ö†Ô∏è  Unable to load model, training a new model...\")\n    \n    def create_ternary_model():\n        model = Sequential([\n            LSTM(64, input_shape=(window_size, len(features)), return_sequences=False),\n            Dropout(0.3),\n            BatchNormalization(),\n            Dense(48, activation='relu'),\n            Dropout(0.2),\n            Dense(32, activation='relu'),\n            Dense(3, activation='softmax')  # Ternary classification output layer\n        ])\n        \n        model.compile(\n            optimizer=Adam(learning_rate=0.0005),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        return model\n    \n    model = create_ternary_model()\n    \n    # Calculate class weights (handle imbalanced data)\n    class_weights = compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(y_train_final),\n        y=y_train_final\n    )\n    class_weight_dict = {i: class_weights[i] for i in range(3)}\n    \n    print(f\"Class Weights: {class_weight_dict}\")\n    \n    # Train the model\n    print(\"Training model...\")\n    history = model.fit(\n        X_train_final, y_train_onehot_final,\n        validation_data=(X_val, y_val_onehot),\n        epochs=30,\n        batch_size=64,\n        verbose=1,\n        class_weight=class_weight_dict,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=8,\n                restore_best_weights=True,\n                verbose=1\n            ),\n            tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=0.5,\n                patience=3,\n                verbose=1\n            )\n        ]\n    )\n\n# Get validation set prediction probabilities for threshold search\ny_val_proba = model.predict(X_val, verbose=0, batch_size=128)\n\n# ============================================================================\n# Optimized Threshold Strategy - Adjusted According to Your Requirements\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Optimized Threshold Strategy\")\nprint(\"1. Default threshold set to 0.4 (ternary classification)\")\nprint(\"2. Validation set goal: Big rise and big drop precision ‚â•60%\")\nprint(\"3. Threshold enumeration step size: 0.01\")\nprint(\"=\"*60)\n\n# ============================================================================\n# 1. Re-search Thresholds (Finer Step Size)\n# ============================================================================\n\nprint(\"\\nRe-searching thresholds, step size: 0.01\")\nrise_thresholds = np.arange(0.3, 0.8, 0.01)  # 0.3 to 0.8, step 0.01\ndrop_thresholds = np.arange(0.3, 0.8, 0.01)\n\nval_results_fine = []\nprint(\"Fine-grained search in progress...\")\n\nfor rise_thresh in rise_thresholds:\n    for drop_thresh in drop_thresholds:\n        # Big rise prediction condition\n        rise_mask = (y_val_proba[:, 1] > rise_thresh) & \\\n                   (y_val_proba[:, 1] > y_val_proba[:, 0]) & \\\n                   (y_val_proba[:, 1] > y_val_proba[:, 2])\n        \n        # Big drop prediction condition\n        drop_mask = (y_val_proba[:, 2] > drop_thresh) & \\\n                   (y_val_proba[:, 2] > y_val_proba[:, 0]) & \\\n                   (y_val_proba[:, 2] > y_val_proba[:, 1])\n        \n        y_val_pred = np.zeros(len(y_val), dtype=int)\n        y_val_pred[rise_mask] = 1\n        y_val_pred[drop_mask] = 2\n        \n        # Calculate metrics\n        rise_indices = np.where(y_val_pred == 1)[0]\n        drop_indices = np.where(y_val_pred == 2)[0]\n        \n        # Big rise metrics\n        if len(rise_indices) > 0:\n            rise_precision = np.mean(y_val[rise_indices] == 1)\n        else:\n            rise_precision = 0\n            \n        # Big drop metrics\n        if len(drop_indices) > 0:\n            drop_precision = np.mean(y_val[drop_indices] == 2)\n        else:\n            drop_precision = 0\n        \n        rise_signals = len(rise_indices)\n        drop_signals = len(drop_indices)\n        total_signals = rise_signals + drop_signals\n        \n        # Calculate recall\n        total_rise = np.sum(y_val == 1)\n        total_drop = np.sum(y_val == 2)\n        \n        if total_rise > 0:\n            rise_recall = np.sum((y_val_pred == 1) & (y_val == 1)) / total_rise\n        else:\n            rise_recall = 0\n            \n        if total_drop > 0:\n            drop_recall = np.sum((y_val_pred == 2) & (y_val == 2)) / total_drop\n        else:\n            drop_recall = 0\n        \n        val_results_fine.append({\n            'rise_threshold': rise_thresh,\n            'drop_threshold': drop_thresh,\n            'rise_precision': rise_precision,\n            'drop_precision': drop_precision,\n            'rise_signals': rise_signals,\n            'drop_signals': drop_signals,\n            'total_signals': total_signals,\n            'rise_recall': rise_recall,\n            'drop_recall': drop_recall,\n            'signal_ratio': total_signals / len(y_val)\n        })\n\nval_results_df_fine = pd.DataFrame(val_results_fine)\n\n# ============================================================================\n# 2. Threshold Selection Strategy (According to Your Requirements)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Threshold Selection Strategy\")\nprint(\"Primary goal: Big rise and big drop precision ‚â•60%\")\nprint(\"Alternative goal: If signals are too few, relax to ‚â•58%\")\nprint(\"=\"*60)\n\n# Strategy 1: Precision ‚â•60%\nthresholds_60 = val_results_df_fine[\n    (val_results_df_fine['rise_precision'] >= 0.60) & \n    (val_results_df_fine['drop_precision'] >= 0.60)\n].copy()\n\nif len(thresholds_60) > 0:\n    print(f\"\\n‚úÖ Found {len(thresholds_60)} threshold combinations with both big rise and big drop precision ‚â•60%\")\n    \n    # Sort by signal count\n    thresholds_60 = thresholds_60.sort_values('total_signals', ascending=False)\n    \n    print(f\"\\nThreshold combinations with precision ‚â•60% (top 10):\")\n    print(f\"{'Rise Thresh':<10} {'Drop Thresh':<10} {'Rise Prec':<12} {'Drop Prec':<12} {'Rise Signals':<12} {'Drop Signals':<12} {'Total Signals':<12}\")\n    print(\"-\" * 90)\n    \n    for i, row in thresholds_60.head(10).iterrows():\n        print(f\"{row['rise_threshold']:.3f}      {row['drop_threshold']:.3f}      \"\n              f\"{row['rise_precision']:.2%}        {row['drop_precision']:.2%}        \"\n              f\"{int(row['rise_signals']):<12} {int(row['drop_signals']):<12} {int(row['total_signals']):<12}\")\n    \n    # Check if signal count is reasonable\n    if thresholds_60.iloc[0]['total_signals'] >= 50:\n        print(f\"\\nStrategy 1A: Sufficient signal count, using ‚â•60% precision combination\")\n        selected_row = thresholds_60.iloc[0]\n        selected_rise_threshold = selected_row['rise_threshold']\n        selected_drop_threshold = selected_row['drop_threshold']\n        selection_strategy = \"60% precision, signal priority\"\n        \n    else:\n        print(f\"\\n‚ö†Ô∏è  Best combination has too few signals ({int(thresholds_60.iloc[0]['total_signals'])}), relaxing to 58%\")\n        \n        # Strategy 2: Precision ‚â•58%\n        thresholds_58 = val_results_df_fine[\n            (val_results_df_fine['rise_precision'] >= 0.58) & \n            (val_results_df_fine['drop_precision'] >= 0.58)\n        ].copy()\n        \n        if len(thresholds_58) > 0:\n            thresholds_58 = thresholds_58.sort_values('total_signals', ascending=False)\n            \n            # Find combinations with signal count ‚â•100\n            good_thresholds_58 = thresholds_58[thresholds_58['total_signals'] >= 100]\n            \n            if len(good_thresholds_58) > 0:\n                print(f\"\\nStrategy 2A: Found 58% precision combination with signals ‚â•100\")\n                selected_row = good_thresholds_58.iloc[0]\n                selection_strategy = \"58% precision, signals ‚â•100\"\n            else:\n                # Select 58% precision combination with most signals\n                print(f\"\\nStrategy 2B: Using 58% precision combination with most signals\")\n                selected_row = thresholds_58.iloc[0]\n                selection_strategy = \"58% precision, most signals\"\n            \n            selected_rise_threshold = selected_row['rise_threshold']\n            selected_drop_threshold = selected_row['drop_threshold']\n            \n        else:\n            print(f\"\\n‚ö†Ô∏è  No combinations found with ‚â•58% precision, using ‚â•55% precision\")\n            \n            # Strategy 3: Precision ‚â•55%\n            thresholds_55 = val_results_df_fine[\n                (val_results_df_fine['rise_precision'] >= 0.55) & \n                (val_results_df_fine['drop_precision'] >= 0.55)\n            ].copy()\n            \n            if len(thresholds_55) > 0:\n                thresholds_55 = thresholds_55.sort_values('total_signals', ascending=False)\n                selected_row = thresholds_55.iloc[0]\n                selected_rise_threshold = selected_row['rise_threshold']\n                selected_drop_threshold = selected_row['drop_threshold']\n                selection_strategy = \"55% precision, most signals\"\n            else:\n                print(f\"\\n‚ùå No combinations found with ‚â•55% precision, using default threshold 0.4\")\n                selected_rise_threshold = 0.4  # Default value\n                selected_drop_threshold = 0.4  # Default value\n                selection_strategy = \"Default threshold 0.4\"\n                \nelse:\n    print(f\"\\n‚ö†Ô∏è  No threshold combinations found with both big rise and big drop precision ‚â•60%\")\n    print(f\"  Maximum big rise precision on validation set: {val_results_df_fine['rise_precision'].max():.2%}\")\n    print(f\"  Maximum big drop precision on validation set: {val_results_df_fine['drop_precision'].max():.2%}\")\n    \n    # Directly try 58% precision\n    print(f\"\\nTrying combinations with precision ‚â•58%...\")\n    thresholds_58 = val_results_df_fine[\n        (val_results_df_fine['rise_precision'] >= 0.58) & \n        (val_results_df_fine['drop_precision'] >= 0.58)\n    ].copy()\n    \n    if len(thresholds_58) > 0:\n        thresholds_58 = thresholds_58.sort_values('total_signals', ascending=False)\n        selected_row = thresholds_58.iloc[0]\n        selected_rise_threshold = selected_row['rise_threshold']\n        selected_drop_threshold = selected_row['drop_threshold']\n        selection_strategy = \"58% precision (no 60% results)\"\n        \n        print(f\"‚úÖ Found ‚â•58% precision combination\")\n        print(f\"  Big rise threshold: {selected_rise_threshold:.3f}\")\n        print(f\"  Big drop threshold: {selected_drop_threshold:.3f}\")\n        print(f\"  Big rise precision: {selected_row['rise_precision']:.2%}\")\n        print(f\"  Big drop precision: {selected_row['drop_precision']:.2%}\")\n        print(f\"  Total signals: {int(selected_row['total_signals'])}\")\n        \n    else:\n        print(f\"‚ö†Ô∏è  No combinations found with ‚â•58% precision, using default threshold 0.4\")\n        selected_rise_threshold = 0.4  # Default value\n        selected_drop_threshold = 0.4  # Default value\n        selection_strategy = \"Default threshold 0.4\"\n\n# ============================================================================\n# 3. Display Final Selected Thresholds\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Final Selected Thresholds\")\nprint(\"=\"*60)\n\nprint(f\"Selection Strategy: {selection_strategy}\")\nprint(f\"Big rise threshold: {selected_rise_threshold:.3f}\")\nprint(f\"Big drop threshold: {selected_drop_threshold:.3f}\")\n\nif selection_strategy != \"Default threshold 0.4\":\n    # Get metrics for selected thresholds\n    mask = (val_results_df_fine['rise_threshold'] == selected_rise_threshold) & \\\n           (val_results_df_fine['drop_threshold'] == selected_drop_threshold)\n    \n    if mask.any():\n        row_data = val_results_df_fine[mask].iloc[0]\n        print(f\"\\nValidation Set Performance:\")\n        print(f\"  Big rise precision: {row_data['rise_precision']:.2%}\")\n        print(f\"  Big drop precision: {row_data['drop_precision']:.2%}\")\n        print(f\"  Big rise signals: {int(row_data['rise_signals'])}\")\n        print(f\"  Big drop signals: {int(row_data['drop_signals'])}\")\n        print(f\"  Total signals: {int(row_data['total_signals'])} ({row_data['signal_ratio']:.2%})\")\n        print(f\"  Big rise recall: {row_data['rise_recall']:.2%}\")\n        print(f\"  Big drop recall: {row_data['drop_recall']:.2%}\")\n\n# ============================================================================\n# 4. Evaluate on Test Set\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test Set Evaluation\")\nprint(\"=\"*60)\n\n# Get test set prediction probabilities\ny_test_proba = model.predict(X_test, verbose=0, batch_size=128)\n\n# Use selected thresholds for prediction\nprint(f\"Using thresholds: Big rise={selected_rise_threshold:.3f}, Big drop={selected_drop_threshold:.3f}\")\n\nrise_mask_test = (y_test_proba[:, 1] > selected_rise_threshold) & \\\n                (y_test_proba[:, 1] > y_test_proba[:, 0]) & \\\n                (y_test_proba[:, 1] > y_test_proba[:, 2])\n\ndrop_mask_test = (y_test_proba[:, 2] > selected_drop_threshold) & \\\n                (y_test_proba[:, 2] > y_test_proba[:, 0]) & \\\n                (y_test_proba[:, 2] > y_test_proba[:, 1])\n\n# Handle conflicts (theoretically shouldn't have conflicts since requiring highest probability)\nconflict_mask_test = rise_mask_test & drop_mask_test\nif np.sum(conflict_mask_test) > 0:\n    print(f\"‚ö†Ô∏è  Found {np.sum(conflict_mask_test)} conflicting samples, handling by probability\")\n    for idx in np.where(conflict_mask_test)[0]:\n        if y_test_proba[idx, 1] > y_test_proba[idx, 2]:\n            drop_mask_test[idx] = False\n        else:\n            rise_mask_test[idx] = False\n\n# Generate final predictions\ny_test_pred = np.zeros(len(y_test_final), dtype=int)\ny_test_pred[rise_mask_test] = 1\ny_test_pred[drop_mask_test] = 2\n\n# Calculate metrics\ntest_accuracy = accuracy_score(y_test_final, y_test_pred)\ntest_precision = precision_score(y_test_final, y_test_pred, average=None, labels=[0, 1, 2], zero_division=0)\ntest_recall = recall_score(y_test_final, y_test_pred, average=None, labels=[0, 1, 2], zero_division=0)\ntest_f1 = f1_score(y_test_final, y_test_pred, average=None, labels=[0, 1, 2], zero_division=0)\n\ntest_rise_signals = np.sum(y_test_pred == 1)\ntest_drop_signals = np.sum(y_test_pred == 2)\ntest_total_signals = test_rise_signals + test_drop_signals\n\nprint(f\"\\nTest Set Performance:\")\nprint(f\"  Accuracy: {test_accuracy:.4f}\")\nprint(f\"  Big rise precision: {test_precision[1]:.4f} (Goal: ‚â•55%)\")\nprint(f\"  Big drop precision: {test_precision[2]:.4f} (Goal: ‚â•55%)\")\nprint(f\"  Big rise recall: {test_recall[1]:.4f}\")\nprint(f\"  Big drop recall: {test_recall[2]:.4f}\")\nprint(f\"  Big rise F1 score: {test_f1[1]:.4f}\")\nprint(f\"  Big drop F1 score: {test_f1[2]:.4f}\")\nprint(f\"  Big rise signal count: {test_rise_signals} ({test_rise_signals/len(y_test_final):.2%})\")\nprint(f\"  Big drop signal count: {test_drop_signals} ({test_drop_signals/len(y_test_final):.2%})\")\nprint(f\"  Total signal count: {test_total_signals} ({test_total_signals/len(y_test_final):.2%})\")\n\n# Check if goal is achieved\ntarget_achieved = (test_precision[1] >= 0.55) and (test_precision[2] >= 0.55)\nrise_achieved = test_precision[1] >= 0.55\ndrop_achieved = test_precision[2] >= 0.55\n\nprint(f\"\\nGoal Achievement Status:\")\nprint(f\"  Big rise precision: {test_precision[1]:.2%} {'‚úì' if rise_achieved else '‚úó'} {'(Achieved)' if rise_achieved else '(Below 55%)'}\")\nprint(f\"  Big drop precision: {test_precision[2]:.2%} {'‚úì' if drop_achieved else '‚úó'} {'(Achieved)' if drop_achieved else '(Below 55%)'}\")\n\nif target_achieved:\n    print(f\"‚úÖ Success! Both big rise and big drop precision ‚â• 55%\")\nelif rise_achieved and not drop_achieved:\n    print(f\"‚ö†Ô∏è  Partial success: Big rise achieved, big drop not achieved\")\n    print(f\"  Suggestion: Can trade big rise, avoid big drop trades\")\nelif drop_achieved and not rise_achieved:\n    print(f\"‚ö†Ô∏è  Partial success: Big drop achieved, big rise not achieved\")\n    print(f\"  Suggestion: Can trade big drop, avoid big rise trades\")\nelse:\n    print(f\"‚ùå Not achieved: Both big rise and big drop precision < 55%\")\n\n# ============================================================================\n# 5. Enhanced Visualization: Final Prediction Results Analysis\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Enhanced Visualization: Final Prediction Results Analysis\")\nprint(\"=\"*60)\n\n# Create a comprehensive visualization figure\nfig = plt.figure(figsize=(20, 16))\nfig.suptitle(f'Ternary Classification Results Analysis\\nRise Threshold: {selected_rise_threshold:.3f}, Drop Threshold: {selected_drop_threshold:.3f}', \n             fontsize=16, fontweight='bold')\n\n# 1. Test Set Probability Distribution by True Class\nax1 = plt.subplot(3, 4, 1)\nfor i, label in enumerate(['Calm', 'Big Rise', 'Big Drop']):\n    probs = y_test_proba[y_test_final == i, i]\n    ax1.hist(probs, bins=30, alpha=0.6, label=f'{label} (True)', density=True)\nax1.set_title('1. True Class Probability Distribution')\nax1.set_xlabel('Predicted Probability')\nax1.set_ylabel('Density')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.axvline(x=selected_rise_threshold, color='green', linestyle='--', alpha=0.7, label=f'Rise Thr={selected_rise_threshold:.3f}')\nax1.axvline(x=selected_drop_threshold, color='red', linestyle='--', alpha=0.7, label=f'Drop Thr={selected_drop_threshold:.3f}')\n\n# 2. Probability Heatmap by True Class\nax2 = plt.subplot(3, 4, 2)\nprobs_by_class = []\nlabels = ['Calm Prob', 'Rise Prob', 'Drop Prob']\nfor i in range(3):\n    probs_by_class.append(y_test_proba[y_test_final == i].mean(axis=0))\nprobs_matrix = np.array(probs_by_class)\nim = ax2.imshow(probs_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\nax2.set_title('2. Average Probability by True Class')\nax2.set_xlabel('Predicted Class Probability')\nax2.set_ylabel('True Class')\nax2.set_xticks(range(3))\nax2.set_yticks(range(3))\nax2.set_xticklabels(labels)\nax2.set_yticklabels(['Calm', 'Rise', 'Drop'])\nfor i in range(3):\n    for j in range(3):\n        text = ax2.text(j, i, f'{probs_matrix[i, j]:.3f}',\n                       ha=\"center\", va=\"center\", color=\"white\" if probs_matrix[i, j] > 0.5 else \"black\")\nplt.colorbar(im, ax=ax2)\n\n# 3. Signal Distribution by Predicted Class\nax3 = plt.subplot(3, 4, 3)\npred_counts = [np.sum(y_test_pred == i) for i in range(3)]\ncolors = ['blue', 'green', 'red']\nbars = ax3.bar(['Calm', 'Rise', 'Drop'], pred_counts, color=colors, alpha=0.7)\nax3.set_title('3. Signal Distribution by Predicted Class')\nax3.set_ylabel('Count')\nax3.set_xlabel('Predicted Class')\nfor bar, count in zip(bars, pred_counts):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height + max(pred_counts)*0.01,\n            f'{count}\\n({count/len(y_test_pred):.1%})', ha='center', va='bottom', fontsize=9)\nax3.grid(True, alpha=0.3)\n\n# 4. Probability vs Threshold Analysis\nax4 = plt.subplot(3, 4, 4)\nthreshold_range = np.linspace(0.1, 0.9, 50)\nrise_signals_by_thresh = []\ndrop_signals_by_thresh = []\nfor thresh in threshold_range:\n    rise_mask = (y_test_proba[:, 1] > thresh)\n    drop_mask = (y_test_proba[:, 2] > thresh)\n    rise_signals_by_thresh.append(np.sum(rise_mask))\n    drop_signals_by_thresh.append(np.sum(drop_mask))\nax4.plot(threshold_range, rise_signals_by_thresh, 'g-', label='Rise Signals', linewidth=2)\nax4.plot(threshold_range, drop_signals_by_thresh, 'r-', label='Drop Signals', linewidth=2)\nax4.axvline(x=selected_rise_threshold, color='green', linestyle='--', alpha=0.7)\nax4.axvline(x=selected_drop_threshold, color='red', linestyle='--', alpha=0.7)\nax4.set_title('4. Signal Count vs Threshold')\nax4.set_xlabel('Threshold')\nax4.set_ylabel('Signal Count')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\n# 5. Confusion Matrix Heatmap\nax5 = plt.subplot(3, 4, 5)\ncm = confusion_matrix(y_test_final, y_test_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax5, \n            xticklabels=['Calm', 'Rise', 'Drop'], \n            yticklabels=['Calm', 'Rise', 'Drop'])\nax5.set_title('5. Confusion Matrix')\nax5.set_xlabel('Predicted')\nax5.set_ylabel('True')\n\n# 6. Precision-Recall by Class\nax6 = plt.subplot(3, 4, 6)\nclasses = ['Calm', 'Rise', 'Drop']\nprecisions = test_precision\nrecalls = test_recall\nx = np.arange(len(classes))\nwidth = 0.35\nbars1 = ax6.bar(x - width/2, precisions, width, label='Precision', alpha=0.7)\nbars2 = ax6.bar(x + width/2, recalls, width, label='Recall', alpha=0.7)\nax6.set_title('6. Precision & Recall by Class')\nax6.set_xlabel('Class')\nax6.set_ylabel('Score')\nax6.set_xticks(x)\nax6.set_xticklabels(classes)\nax6.axhline(y=0.55, color='red', linestyle='--', alpha=0.5, label='55% Target')\nax6.legend()\nax6.grid(True, alpha=0.3)\n\n# 7. Probability Distribution for Rise Predictions\nax7 = plt.subplot(3, 4, 7)\nrise_indices = np.where(y_test_pred == 1)[0]\nif len(rise_indices) > 0:\n    true_rise = y_test_final[rise_indices] == 1\n    false_rise = ~true_rise\n    if np.sum(true_rise) > 0:\n        ax7.hist(y_test_proba[rise_indices][true_rise, 1], bins=20, alpha=0.7, \n                color='green', label='True Rise', density=True)\n    if np.sum(false_rise) > 0:\n        ax7.hist(y_test_proba[rise_indices][false_rise, 1], bins=20, alpha=0.7, \n                color='orange', label='False Rise', density=True)\n    ax7.axvline(x=selected_rise_threshold, color='darkgreen', linestyle='--', linewidth=2)\n    ax7.set_title(f'7. Rise Predictions (Precision: {test_precision[1]:.2%})')\n    ax7.set_xlabel('Rise Probability')\n    ax7.set_ylabel('Density')\n    ax7.legend()\n    ax7.grid(True, alpha=0.3)\n\n# 8. Probability Distribution for Drop Predictions\nax8 = plt.subplot(3, 4, 8)\ndrop_indices = np.where(y_test_pred == 2)[0]\nif len(drop_indices) > 0:\n    true_drop = y_test_final[drop_indices] == 2\n    false_drop = ~true_drop\n    if np.sum(true_drop) > 0:\n        ax8.hist(y_test_proba[drop_indices][true_drop, 2], bins=20, alpha=0.7, \n                color='red', label='True Drop', density=True)\n    if np.sum(false_drop) > 0:\n        ax8.hist(y_test_proba[drop_indices][false_drop, 2], bins=20, alpha=0.7, \n                color='orange', label='False Drop', density=True)\n    ax8.axvline(x=selected_drop_threshold, color='darkred', linestyle='--', linewidth=2)\n    ax8.set_title(f'8. Drop Predictions (Precision: {test_precision[2]:.2%})')\n    ax8.set_xlabel('Drop Probability')\n    ax8.set_ylabel('Density')\n    ax8.legend()\n    ax8.grid(True, alpha=0.3)\n\n# 9. 3D Probability Scatter Plot (simplified 2D projection)\nax9 = plt.subplot(3, 4, 9)\n# Plot rise probability vs drop probability\nscatter = ax9.scatter(y_test_proba[:, 1], y_test_proba[:, 2], \n                     c=y_test_pred, cmap='viridis', alpha=0.6, s=10)\nax9.set_title('9. Rise vs Drop Probability Scatter')\nax9.set_xlabel('Rise Probability')\nax9.set_ylabel('Drop Probability')\nax9.axvline(x=selected_rise_threshold, color='green', linestyle='--', alpha=0.5)\nax9.axhline(y=selected_drop_threshold, color='red', linestyle='--', alpha=0.5)\nax9.grid(True, alpha=0.3)\n\n# 10. Performance Summary Bar Chart\nax10 = plt.subplot(3, 4, 10)\nmetrics = ['Accuracy', 'Rise Prec', 'Drop Prec', 'Rise Rec', 'Drop Rec']\nvalues = [test_accuracy, test_precision[1], test_precision[2], \n          test_recall[1], test_recall[2]]\ncolors_metric = ['blue', 'lightgreen', 'lightcoral', 'green', 'red']\nbars10 = ax10.bar(metrics, values, color=colors_metric, alpha=0.7)\nax10.set_title('10. Performance Metrics Summary')\nax10.set_ylabel('Score')\nax10.axhline(y=0.55, color='red', linestyle='--', alpha=0.5, label='55% Target')\nfor bar, value in zip(bars10, values):\n    height = bar.get_height()\n    ax10.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n             f'{value:.3f}', ha='center', va='bottom', fontsize=9)\nax10.set_ylim([0, 1.1])\nax10.grid(True, alpha=0.3)\n\n# 11. Signal Quality Analysis\nax11 = plt.subplot(3, 4, 11)\nif test_rise_signals > 0 and test_drop_signals > 0:\n    signal_data = {\n        'Rise': [test_rise_signals, test_precision[1]],\n        'Drop': [test_drop_signals, test_precision[2]]\n    }\n    x = np.arange(2)\n    width = 0.35\n    \n    # Signal count bars\n    counts = [signal_data['Rise'][0], signal_data['Drop'][0]]\n    bars_counts = ax11.bar(x - width/2, counts, width, label='Signal Count', alpha=0.7, color=['lightgreen', 'lightcoral'])\n    \n    # Precision bars\n    precs = [signal_data['Rise'][1], signal_data['Drop'][1]]\n    bars_precs = ax11.bar(x + width/2, precs, width, label='Precision', alpha=0.7, color=['green', 'red'])\n    \n    ax11.set_title('11. Signal Quality Analysis')\n    ax11.set_xlabel('Signal Type')\n    ax11.set_ylabel('Value')\n    ax11.set_xticks(x)\n    ax11.set_xticklabels(['Rise', 'Drop'])\n    ax11.axhline(y=0.55, color='red', linestyle='--', alpha=0.5)\n    ax11.legend()\n    ax11.grid(True, alpha=0.3)\n\n# 12. Model Confidence Distribution\nax12 = plt.subplot(3, 4, 12)\nmax_probs = np.max(y_test_proba, axis=1)\nconfidence_bins = [0, 0.3, 0.5, 0.7, 0.9, 1.0]\nconfidence_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\nconfidence_counts, _ = np.histogram(max_probs, bins=confidence_bins)\nax12.pie(confidence_counts, labels=confidence_labels, autopct='%1.1f%%', \n        colors=['red', 'orange', 'yellow', 'lightgreen', 'green'])\nax12.set_title('12. Model Confidence Distribution')\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# Additional Detailed Analysis Charts\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Additional Detailed Analysis\")\nprint(\"=\"*60)\n\n# Create second figure for detailed analysis\nfig2, axes2 = plt.subplots(2, 3, figsize=(15, 10))\nfig2.suptitle('Detailed Prediction Analysis', fontsize=14, fontweight='bold')\n\n# 1. Class-wise ROC-like curves (probability distributions)\nax1 = axes2[0, 0]\nfor true_class in range(3):\n    for pred_class in range(3):\n        if true_class != pred_class:\n            continue\n        probs = y_test_proba[y_test_final == true_class, pred_class]\n        if len(probs) > 0:\n            ax1.hist(probs, bins=30, alpha=0.5, \n                    label=f'True {true_class} -> Pred {pred_class}', density=True)\nax1.set_title('1. Correct Prediction Probability Distribution')\nax1.set_xlabel('Probability')\nax1.set_ylabel('Density')\nax1.legend(fontsize=8)\nax1.grid(True, alpha=0.3)\n\n# 2. Error analysis: misclassified samples\nax2 = axes2[0, 1]\nmisclassified_mask = y_test_pred != y_test_final\nif np.sum(misclassified_mask) > 0:\n    misclassified_probs = y_test_proba[misclassified_mask]\n    misclassified_true = y_test_final[misclassified_mask]\n    misclassified_pred = y_test_pred[misclassified_mask]\n    \n    error_types = []\n    for true_label, pred_label in zip(misclassified_true, misclassified_pred):\n        error_types.append(f'{true_label}->{pred_label}')\n    \n    unique_errors, error_counts = np.unique(error_types, return_counts=True)\n    colors_error = plt.cm.Set3(np.linspace(0, 1, len(unique_errors)))\n    ax2.pie(error_counts, labels=unique_errors, autopct='%1.1f%%', colors=colors_error)\n    ax2.set_title('2. Error Type Distribution')\n\n# 3. Threshold sensitivity analysis\nax3 = axes2[0, 2]\ntest_thresholds = np.linspace(0.3, 0.7, 20)\nrise_precisions = []\ndrop_precisions = []\nfor thresh in test_thresholds:\n    # Test with same threshold for both rise and drop\n    rise_mask = (y_test_proba[:, 1] > thresh) & (y_test_proba[:, 1] > y_test_proba[:, 0]) & (y_test_proba[:, 1] > y_test_proba[:, 2])\n    drop_mask = (y_test_proba[:, 2] > thresh) & (y_test_proba[:, 2] > y_test_proba[:, 0]) & (y_test_proba[:, 2] > y_test_proba[:, 1])\n    \n    if np.sum(rise_mask) > 0:\n        rise_prec = np.mean(y_test_final[rise_mask] == 1)\n    else:\n        rise_prec = 0\n    \n    if np.sum(drop_mask) > 0:\n        drop_prec = np.mean(y_test_final[drop_mask] == 2)\n    else:\n        drop_prec = 0\n    \n    rise_precisions.append(rise_prec)\n    drop_precisions.append(drop_prec)\n\nax3.plot(test_thresholds, rise_precisions, 'g-', label='Rise Precision', linewidth=2)\nax3.plot(test_thresholds, drop_precisions, 'r-', label='Drop Precision', linewidth=2)\nax3.axvline(x=selected_rise_threshold, color='green', linestyle='--', alpha=0.7)\nax3.axvline(x=selected_drop_threshold, color='red', linestyle='--', alpha=0.7)\nax3.axhline(y=0.55, color='black', linestyle='--', alpha=0.5, label='55% Target')\nax3.set_title('3. Threshold Sensitivity Analysis')\nax3.set_xlabel('Threshold')\nax3.set_ylabel('Precision')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 4. Signal timing analysis (if time series data)\nax4 = axes2[1, 0]\nif hasattr(X_test, 'shape') and len(X_test.shape) > 2:\n    # Assuming X_test has time dimension\n    signal_positions = np.where(y_test_pred > 0)[0]\n    if len(signal_positions) > 0:\n        ax4.hist(signal_positions, bins=30, alpha=0.7, color='purple')\n        ax4.set_title('4. Signal Distribution Over Time')\n        ax4.set_xlabel('Time Position')\n        ax4.set_ylabel('Signal Count')\n        ax4.grid(True, alpha=0.3)\n\n# 5. Model confidence vs accuracy\nax5 = axes2[1, 1]\nconfidence_bins = 10\nbin_edges = np.linspace(0, 1, confidence_bins + 1)\naccuracies = []\nconfidence_midpoints = []\nfor i in range(confidence_bins):\n    mask = (max_probs >= bin_edges[i]) & (max_probs < bin_edges[i+1])\n    if np.sum(mask) > 0:\n        accuracy = np.mean(y_test_pred[mask] == y_test_final[mask])\n        accuracies.append(accuracy)\n        confidence_midpoints.append((bin_edges[i] + bin_edges[i+1]) / 2)\n\nax5.plot(confidence_midpoints, accuracies, 'b-o', linewidth=2)\nax5.set_title('5. Model Confidence vs Accuracy')\nax5.set_xlabel('Model Confidence (Max Probability)')\nax5.set_ylabel('Accuracy')\nax5.grid(True, alpha=0.3)\nax5.set_ylim([0, 1.1])\n\n# 6. Summary statistics table\nax6 = axes2[1, 2]\nax6.axis('tight')\nax6.axis('off')\nsummary_text = f\"\"\"\nModel Performance Summary:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nThreshold Strategy: {selection_strategy}\nRise Threshold: {selected_rise_threshold:.3f}\nDrop Threshold: {selected_drop_threshold:.3f}\n\nPerformance Metrics:\n‚Ä¢ Overall Accuracy: {test_accuracy:.2%}\n‚Ä¢ Rise Precision: {test_precision[1]:.2%} {'‚úì' if test_precision[1] >= 0.55 else '‚úó'}\n‚Ä¢ Drop Precision: {test_precision[2]:.2%} {'‚úì' if test_precision[2] >= 0.55 else '‚úó'}\n‚Ä¢ Rise Recall: {test_recall[1]:.2%}\n‚Ä¢ Drop Recall: {test_recall[2]:.2%}\n\nSignal Statistics:\n‚Ä¢ Total Signals: {test_total_signals}\n‚Ä¢ Rise Signals: {test_rise_signals} ({test_rise_signals/len(y_test_final):.2%})\n‚Ä¢ Drop Signals: {test_drop_signals} ({test_drop_signals/len(y_test_final):.2%})\n\nTrading Potential:\n‚Ä¢ Annual Trades: {annual_trades:.0f}\n‚Ä¢ Target Achieved: {'Yes' if target_achieved else 'Partial' if (rise_achieved or drop_achieved) else 'No'}\n\"\"\"\nax6.text(0.1, 0.9, summary_text, fontsize=9, family='monospace',\n        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# 6. Final Analysis and Recommendations\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Final Analysis and Recommendations\")\nprint(\"=\"*60)\n\nprint(f\"\\nüìä Final Performance Summary:\")\nprint(f\"  Threshold selection strategy: {selection_strategy}\")\nprint(f\"  Big rise threshold: {selected_rise_threshold:.3f}\")\nprint(f\"  Big drop threshold: {selected_drop_threshold:.3f}\")\nprint(f\"  Big rise precision: {test_precision[1]:.2%} {'(Achieved)' if test_precision[1] >= 0.55 else '(Not achieved)'}\")\nprint(f\"  Big drop precision: {test_precision[2]:.2%} {'(Achieved)' if test_precision[2] >= 0.55 else '(Not achieved)'}\")\nprint(f\"  Big rise signal count: {test_rise_signals} ({test_rise_signals/len(y_test_final):.2%})\")\nprint(f\"  Big drop signal count: {test_drop_signals} ({test_drop_signals/len(y_test_final):.2%})\")\nprint(f\"  Total signal count: {test_total_signals} ({test_total_signals/len(y_test_final):.2%})\")\n\n# Calculate annualized trade count\nannual_trades = test_total_signals / len(y_test_final) * 252\nprint(f\"  Estimated annual trade count: {annual_trades:.0f} trades\")\n\nprint(f\"\\nüí° Trading Recommendations:\")\nif test_total_signals == 0:\n    print(f\"  ‚ùå No trading signals, model needs retraining\")\nelif test_total_signals < 20:\n    print(f\"  ‚ö†Ô∏è  Too few signals ({test_total_signals}), not recommended for actual trading\")\nelif test_total_signals >= 20 and test_total_signals < 100:\n    print(f\"  ‚ö° Moderate signals ({test_total_signals}), recommended for small position testing\")\n    if test_precision[1] >= 0.55 and test_precision[2] >= 0.55:\n        print(f\"    Two-way trading, position: 1-2%\")\n    elif test_precision[1] >= 0.55:\n        print(f\"    Focus on big rise trading, position: 2-3%\")\n    elif test_precision[2] >= 0.55:\n        print(f\"    Focus on big drop trading, position: 2-3%\")\nelse:\n    print(f\"  ‚úÖ Sufficient signals ({test_total_signals}), can be used for regular trading\")\n    if test_precision[1] >= 0.55 and test_precision[2] >= 0.55:\n        print(f\"    Two-way trading, position: 2-3%\")\n    elif test_precision[1] >= 0.55:\n        print(f\"    Focus on big rise trading, position: 3-4%\")\n    elif test_precision[2] >= 0.55:\n        print(f\"    Focus on big drop trading, position: 3-4%\")\n\nprint(f\"\\nüìà Visualization Insights:\")\nprint(f\"  1. Probability Distribution: Check if predictions are well-calibrated\")\nprint(f\"  2. Confusion Matrix: Identify which errors are most common\")\nprint(f\"  3. Threshold Sensitivity: Shows how precision changes with threshold\")\nprint(f\"  4. Model Confidence: High confidence ‚â† high accuracy (check Chart 5)\")\n\nprint(f\"\\nüîß Model Usage Instructions:\")\nprint(f\"  After loading model, predict on new data:\")\nprint(f\"  probabilities = model.predict(X_new)\")\nprint(f\"  # Generate big rise signals\")\nprint(f\"  rise_signals = (probabilities[:, 1] > {selected_rise_threshold:.3f}) & \\\\\")\nprint(f\"                 (probabilities[:, 1] > probabilities[:, 0]) & \\\\\")\nprint(f\"                 (probabilities[:, 1] > probabilities[:, 2])\")\nprint(f\"  # Generate big drop signals\")\nprint(f\"  drop_signals = (probabilities[:, 2] > {selected_drop_threshold:.3f}) & \\\\\")\nprint(f\"                 (probabilities[:, 2] > probabilities[:, 0]) & \\\\\")\nprint(f\"                 (probabilities[:, 2] > probabilities[:, 1])\")\n\nprint(f\"\\n‚úÖ Enhanced visualization and threshold optimization completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T18:06:40.841539Z","iopub.execute_input":"2026-01-09T18:06:40.841961Z","iopub.status.idle":"2026-01-09T18:10:08.442177Z","shell.execute_reply.started":"2026-01-09T18:06:40.841932Z","shell.execute_reply":"2026-01-09T18:10:08.441311Z"}},"outputs":[],"execution_count":null}]}